{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Lane Follow model - Nvidia CNN model",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kon0v3/dmcar-ai/blob/main/1_Lane_Follow_model_Nvidia_CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OrScureymFU"
      },
      "source": [
        "# Lane Follow using Nvidia CNN model\n",
        "This lab is to create \"Nvidia CNN model\" for Autonomous Car project in AtoI:\n",
        "*   Customized Traning model\n",
        "*   Original Model: Nvidia CNN\n",
        "*   Output Model: \"lane_navigation_check.h5\" for dmcar_model.py\n",
        "*   Move *.h5 file to models directory and rename as \"lane.model\"\n",
        "\n",
        "The model is based on a Nvidia Convolutional Neural Network (CNN) model. At the core of the Nvidia model, there is a Convolutional Neural Network. CNNs are used prevalently in image recognition deep learning models. The intuition is that CNN is especially good at extracting visual features from images from its various layers (a.k.a. filters). "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, you need to adjust python packages (tensorflow and keras). After run the following cell, go to \"Runtime\" menu, then click \"Restart Runtime\"."
      ],
      "metadata": {
        "id": "7S4jaCzH9gr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow==1.14.0\n",
        "! pip install tensorboard==2.0.2\n",
        "! pip install Keras==2.3.1\n",
        "! pip install h5py==2.10.0"
      ],
      "metadata": {
        "id": "Hlh5Rmvq9dRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFZEdyZPtdMG"
      },
      "source": [
        "## Imports Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-BJrGUh00Cp"
      },
      "source": [
        "# python standard libraries\n",
        "import os\n",
        "import random\n",
        "import fnmatch\n",
        "import datetime\n",
        "import pickle\n",
        "\n",
        "# data processing\n",
        "import numpy as np\n",
        "np.set_printoptions(formatter={'float_kind':lambda x: \"%.4f\" % x})\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.width', 300)\n",
        "pd.set_option('display.float_format', '{:,.4f}'.format)\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential  # V2 is tensorflow.keras.xxxx, V1 is keras.xxx\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
        "#from keras.optimizers import Adam\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "\n",
        "#print( f'tf.__version__: {tf.__version__}' )\n",
        "#print( f'keras.__version__: {keras.__version__}' )\n",
        "\n",
        "# sklearn\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# imaging\n",
        "import cv2\n",
        "from imgaug import augmenters as img_aug\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "from PIL import Image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At4UECHhz9Pp"
      },
      "source": [
        "## Load Train Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdnP0Yvoot5t"
      },
      "source": [
        "Before you start this lab, you need create and upload a dataset on your Google Drive by following:\n",
        "1.   Use the same dataset you created for \"lane.model\" for Lane Detection and Follow\n",
        "2.   On your car, go to \"model_lane_follow\" folder\n",
        "3.   Create dataset file\n",
        "```\n",
        "tar cvzf train_data.tgz train_data\n",
        "```\n",
        "4.   Upload train_data.tgz file to Google Drive \"data\" folder. (If you do not have \"data\" folder, you need to creat it first)\n",
        "\n",
        "Pay attention to this part so you can reproduce it with your own train_data dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rihBIMTlM7rm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! rm -rf train_data\n",
        "! rm -rf logs\n",
        "! rm -rf train_model\n",
        "! tar -xvzf \"/content/drive/My Drive/data/train_data.tgz\"\n",
        "! mkdir logs\n",
        "! mkdir train_model\n",
        "data_dir = '/content/train_data'\n",
        "log_dir_root = '/content/logs/'\n",
        "model_output_dir = '/content/train_model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q29fijIfVZYv"
      },
      "source": [
        "file_list = os.listdir(data_dir)\n",
        "image_paths = []\n",
        "steering_angles = []\n",
        "pattern = \"*.png\"\n",
        "for filename in file_list:\n",
        "    if fnmatch.fnmatch(filename, pattern):\n",
        "        image_paths.append(os.path.join(data_dir,filename))\n",
        "        angle = int(filename[-7:-4])  # 092 part of video01_143_092.png is the angle. 90 is go straight\n",
        "        steering_angles.append(angle)\n",
        "\n",
        "image_index = 20\n",
        "plt.imshow(Image.open(image_paths[image_index]))\n",
        "print(\"image_path: %s\" % image_paths[image_index] )\n",
        "print(\"steering_Angle: %d\" % steering_angles[image_index] )\n",
        "df = pd.DataFrame()\n",
        "df['ImagePath'] = image_paths\n",
        "df['Angle'] = steering_angles\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhsGlMrwVdE2"
      },
      "source": [
        "# Look at the distribution of steering angle\n",
        "num_of_bins = 25\n",
        "samples_per_bin = 400\n",
        "hist, bins = np.histogram(df['Angle'], num_of_bins)\n",
        "\n",
        "fig, axes = plt.subplots(1,1, figsize=(12,4))\n",
        "axes.hist(df['Angle'], bins=num_of_bins, width=1, color='blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc_Z4szldV8K"
      },
      "source": [
        "Notice that the above diagram contains angeles mostly higher than 90 or smaller than 90 if you used one-way turn to generate dataset.  This makes sense, because in our training data, PiCar was mostly turning left or right only.  This is going to fine, because we will balance the data by randomly flip the image, and the steering angle in the image generator process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPqHnXq1gWRC"
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split( image_paths, steering_angles, test_size=0.2)\n",
        "print(\"Training data: %d\\nValidation data: %d\" % (len(X_train), len(X_valid)))\n",
        "\n",
        "# plot the distributions of train and valid, make sure they are consistent\n",
        "fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
        "axes[0].hist(y_train, bins=num_of_bins, width=1, color='blue')\n",
        "axes[0].set_title('Training Data')\n",
        "axes[1].hist(y_valid, bins=num_of_bins, width=1, color='red')\n",
        "axes[1].set_title('Validation Data')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaZEfJxdi796"
      },
      "source": [
        "##Image Augumentation\n",
        "Since we only have a few hundred images, to train a deep network, we need a lot more images.   Instead of running our car, let's try to augment our data. There are a couple of ways to do that.\n",
        "\n",
        "1. Zoom: crop out a smaller image from the center\n",
        "1. Pan: crop out a smaller image from left or right side\n",
        "1. Adjust brightness of the image\n",
        "1. Flip the image horizontally, i.e do a left to right flip, and change the steering angle coorespondingly\n",
        "1. Introduce an Gaussian blur\n",
        "\n",
        "We can combine the above augmentation techniques to generate 100s times of the training images, with just a few hundred real images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyx0sL2ekZoi"
      },
      "source": [
        "def my_imread(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "def zoom(image):\n",
        "    zoom = img_aug.Affine(scale=(1, 1.3))  # zoom from 100% (no zoom) to 130%\n",
        "    image = zoom.augment_image(image)\n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_zoom = zoom(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_zoom)\n",
        "axes[1].set_title(\"zoomed\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ZAbxCDmnJM"
      },
      "source": [
        "def pan(image):\n",
        "    # pan left / right / up / down about 10%\n",
        "    pan = img_aug.Affine(translate_percent= {\"x\" : (-0.1, 0.1), \"y\": (-0.1, 0.1)})\n",
        "    image = pan.augment_image(image)\n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_pan = pan(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_pan)\n",
        "axes[1].set_title(\"panned\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eggszMEUnKHQ"
      },
      "source": [
        "def adjust_brightness(image):\n",
        "    # increase or decrease brightness by 30%\n",
        "    brightness = img_aug.Multiply((0.7, 1.3))\n",
        "    image = brightness.augment_image(image)\n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_brightness = adjust_brightness(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_brightness)\n",
        "axes[1].set_title(\"brightness adjusted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAtWi8ix-L7R"
      },
      "source": [
        "def blur(image):\n",
        "    kernel_size = random.randint(1, 5)  # kernel larger than 5 would make the image way too blurry\n",
        "    image = cv2.blur(image,(kernel_size, kernel_size))\n",
        "   \n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_blur = blur(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_blur)\n",
        "axes[1].set_title(\"blurred\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ags51OwPn3qR"
      },
      "source": [
        "def random_flip(image, steering_angle):\n",
        "    is_flip = random.randint(0, 1)\n",
        "    if is_flip == 1:\n",
        "        # randomly flip horizon\n",
        "        image = cv2.flip(image,1)\n",
        "        steering_angle = 180 - steering_angle\n",
        "   \n",
        "    return image, steering_angle\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_flip, steering_angle = random_flip(image_orig, steering_angles[image_index])\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_flip)\n",
        "axes[1].set_title(\"flipped, angle=%s\" % steering_angle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdQIzXiuo5zD"
      },
      "source": [
        "# put it together\n",
        "def random_augment(image, steering_angle):\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = pan(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = zoom(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = blur(image)\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = adjust_brightness(image)\n",
        "    image, steering_angle = random_flip(image, steering_angle)\n",
        "    \n",
        "    return image, steering_angle\n",
        "\n",
        "# show a few randomly augmented images\n",
        "ncol = 2\n",
        "nrow = 10\n",
        "fig, axes = plt.subplots(nrow, ncol, figsize=(15, 50))\n",
        "\n",
        "for i in range(nrow):\n",
        "    rand_index = random.randint(0, len(image_paths) - 1)\n",
        "    image_path = image_paths[rand_index]\n",
        "    steering_angle_orig = steering_angles[rand_index]\n",
        "    \n",
        "    image_orig = my_imread(image_path)\n",
        "    image_aug, steering_angle_aug = random_augment(image_orig, steering_angle_orig)\n",
        "    \n",
        "    axes[i][0].imshow(image_orig)\n",
        "    axes[i][0].set_title(\"original, angle=%s\" % steering_angle_orig)\n",
        "    axes[i][1].imshow(image_aug)\n",
        "    axes[i][1].set_title(\"augmented, angle=%s\" % steering_angle_aug)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xCtiEgo0C4S"
      },
      "source": [
        "## Preprocess Training Data for Nvidia Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45-dWwTw0K5x"
      },
      "source": [
        "def img_preprocess(image):\n",
        "    height, _, _ = image.shape\n",
        "    image = image[int(height/2):,:,:]   # remove top half of the image, as it is not relavant for lane following\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)  # Nvidia model said it is best to use YUV color space\n",
        "    image = cv2.GaussianBlur(image, (3,3), 0)\n",
        "    image = cv2.resize(image, (200,66)) # input image size (200,66) Nvidia model\n",
        "    image = image / 255                 # normalizing\n",
        "    return image\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "image_orig = my_imread(image_paths[image_index])\n",
        "image_processed = img_preprocess(image_orig)\n",
        "axes[0].imshow(image_orig)\n",
        "axes[0].set_title(\"orig\")\n",
        "axes[1].imshow(image_processed)\n",
        "axes[1].set_title(\"processed\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "envGeErj0LfP"
      },
      "source": [
        "## Create and Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_mnzbPi7xPo"
      },
      "source": [
        "This is the Nvidia CNN Model Architecture. The input layer is at the bottom with size of 200x66 in YUV color space ![](https://github.com/dctian/DeepPiCar/raw/master/models/lane_navigation/doc/NVidia%20Model%20Architecture.JPG) .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB8_GDbn0VX4"
      },
      "source": [
        "def nvidia_model():\n",
        "    model = Sequential(name='Nvidia_Model')\n",
        "    \n",
        "    # elu=Expenential Linear Unit, similar to leaky Relu\n",
        "    \n",
        "    # Convolution Layers\n",
        "    model.add(Conv2D(24, (5, 5), strides=(2, 2), input_shape=(66, 200, 3), activation='elu')) \n",
        "    model.add(Conv2D(36, (5, 5), strides=(2, 2), activation='elu')) \n",
        "    model.add(Conv2D(48, (5, 5), strides=(2, 2), activation='elu')) \n",
        "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
        "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
        "    model.add(Conv2D(64, (3, 3), activation='elu')) \n",
        "    \n",
        "    # Fully Connected Layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.2)) # not in original model. added for more robustness\n",
        "    model.add(Dense(100, activation='elu'))\n",
        "    model.add(Dense(50, activation='elu'))\n",
        "    model.add(Dense(10, activation='elu'))\n",
        "    \n",
        "    # output layer: turn angle (from 45-135, 90 is straight, <90 turn left, >90 turn right)\n",
        "    model.add(Dense(1)) \n",
        "    \n",
        "    # since this is a regression problem not classification problem,\n",
        "    # we use MSE (Mean Squared Error) as loss function\n",
        "    optimizer = Adam(learning_rate=1e-3)\n",
        "    model.compile(loss='mse', optimizer=optimizer)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-FjHgMhA2vl"
      },
      "source": [
        "model = nvidia_model()\n",
        "print(model.summary())\n",
        "# check at we will have 252,219 trainable parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQwQSh5fzEwy"
      },
      "source": [
        "def image_data_generator(image_paths, steering_angles, batch_size, is_training):\n",
        "    while True:\n",
        "        batch_images = []\n",
        "        batch_steering_angles = []\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            random_index = random.randint(0, len(image_paths) - 1)\n",
        "            image_path = image_paths[random_index]\n",
        "            image = my_imread(image_paths[random_index])\n",
        "            steering_angle = steering_angles[random_index]\n",
        "            if is_training:\n",
        "                # training: augment image\n",
        "                image, steering_angle = random_augment(image, steering_angle)\n",
        "              \n",
        "            image = img_preprocess(image)\n",
        "            batch_images.append(image)\n",
        "            batch_steering_angles.append(steering_angle)\n",
        "            \n",
        "        yield( np.asarray(batch_images), np.asarray(batch_steering_angles))\n",
        "            \n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rvVKC_M1kUu"
      },
      "source": [
        "ncol = 2\n",
        "nrow = 2\n",
        "\n",
        "X_train_batch, y_train_batch = next(image_data_generator(X_train, y_train, nrow, True))\n",
        "X_valid_batch, y_valid_batch = next(image_data_generator(X_valid, y_valid, nrow, False))\n",
        "\n",
        "fig, axes = plt.subplots(nrow, ncol, figsize=(15, 6))\n",
        "fig.tight_layout()\n",
        "\n",
        "for i in range(nrow):\n",
        "    axes[i][0].imshow(X_train_batch[i])\n",
        "    axes[i][0].set_title(\"training, angle=%s\" % y_train_batch[i])\n",
        "    axes[i][1].imshow(X_valid_batch[i])\n",
        "    axes[i][1].set_title(\"validation, angle=%s\" % y_valid_batch[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTbnRsnQuM_o"
      },
      "source": [
        "Now, you are ready to generate a model. This wil take 40 ~ 50 minutes. If you want to reduce the time, you can adjust the followings:\n",
        "-   steps_per_epoch=300\n",
        "-   epochs=10\n",
        "-   validation_steps=200\n",
        "-   batch_size=100\n",
        "\n",
        "However, the accuracy and loss will be vary depending on the changed values.\n",
        "\n",
        "To save the running time, you can use the following options (but less accurate):\n",
        "-   steps_per_epoch=100\n",
        "-   epochs=10\n",
        "-   validation_steps=100\n",
        "-   batch_size=20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cLqWYTGA3PG"
      },
      "source": [
        "# saves the model weights after each epoch if the validation loss decreased\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=os.path.join(model_output_dir,'lane_navigation_check.h5'), verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(image_data_generator( X_train, y_train, batch_size=20, is_training=True),\n",
        "                              steps_per_epoch=100,\n",
        "                              epochs=10,\n",
        "                              validation_data = image_data_generator( X_valid, y_valid, batch_size=20, is_training=False),\n",
        "                              validation_steps=100,\n",
        "                              verbose=1,\n",
        "                              shuffle=1,\n",
        "                              callbacks=[checkpoint_callback])\n",
        "# always save model output as soon as model finishes training\n",
        "model.save(os.path.join(model_output_dir,'lane_navigation_final.h5'))\n",
        "\n",
        "date_str = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "history_path = os.path.join(model_output_dir,'history.pickle')\n",
        "with open(history_path, 'wb') as f:\n",
        "    pickle.dump(history.history, f, pickle.HIGHEST_PROTOCOL)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MB5VJmU0Vqu"
      },
      "source": [
        "## Check Trained Model on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUNqiczguJx4"
      },
      "source": [
        "history.history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf8ls0EodC4v"
      },
      "source": [
        "# plot training and validation losses\n",
        "# this should be the same as tensorboard\n",
        "history_path = os.path.join(model_output_dir,'history.pickle')\n",
        "with open(history_path, 'rb') as f:\n",
        "    history = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH_xNvr_A3ak"
      },
      "source": [
        "history\n",
        "plt.plot(history['loss'],color='blue')\n",
        "plt.plot(history['val_loss'],color='red')\n",
        "plt.legend([\"training loss\", \"validation loss\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7IrK0KPf8uk"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def summarize_prediction(Y_true, Y_pred):\n",
        "    \n",
        "    mse = mean_squared_error(Y_true, Y_pred)\n",
        "    r_squared = r2_score(Y_true, Y_pred)\n",
        "    \n",
        "    print(f'mse       = {mse:.2}')\n",
        "    print(f'r_squared = {r_squared:.2%}')\n",
        "    print()\n",
        "    \n",
        "def predict_and_summarize(X, Y):\n",
        "    model = load_model(f'{model_output_dir}/lane_navigation_check.h5')\n",
        "    Y_pred = model.predict(X)\n",
        "    summarize_prediction(Y, Y_pred)\n",
        "    return Y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g238xj_XhymB"
      },
      "source": [
        "n_tests = 100\n",
        "X_test, y_test = next(image_data_generator(X_valid, y_valid, 100, False))\n",
        "\n",
        "y_pred = predict_and_summarize(X_test, y_test)\n",
        "\n",
        "n_tests_show = 2\n",
        "fig, axes = plt.subplots(n_tests_show, 1, figsize=(10, 4 * n_tests_show))\n",
        "for i in range(n_tests_show):\n",
        "    axes[i].imshow(X_test[i])\n",
        "    axes[i].set_title(f\"actual angle={y_test[i]}, predicted angle={int(y_pred[i])}, diff = {int(y_pred[i])-y_test[i]}\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZuSW6vknulN"
      },
      "source": [
        "As we can see the $R^2$ of the predicted was around 95% and Mean Squared Errors (MSE) are low, indicating the model is predicting a steering angle every simliar to our hand coded land follower, which was used as the model input. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "XUS6xs9wEjy8",
        "outputId": "46707bd8-ca56-40b6-bcff-015526ce1a40"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(os.path.join(model_output_dir,'lane_navigation_final.h5'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ed0b59b1-1823-45d8-b2bf-d56988d14f02\", \"lane_navigation_final.h5\", 3107496)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-09XwaJyvG7"
      },
      "source": [
        "## References\n",
        "1. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba (2016) *End to End Learning for Self-Driving Cars*. Nvidia \n",
        "1. Rayan Slim, Amer Sharaf, Jad Slim (2017) *The Complete Self-Driving Car Course*. Udemy\n",
        "1. Keras Documentation (2019) https://github.com/keras-team/keras/\n",
        "\n"
      ]
    }
  ]
}